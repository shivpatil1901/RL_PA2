{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GYzWwVq-jzHr",
        "outputId": "5ee90384-b794-45c3-dd63-205b6b70c388"
      },
      "outputs": [],
      "source": [
        "!pip install optuna"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Defining the Neural Networks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dDwQPFEE2D3C"
      },
      "outputs": [],
      "source": [
        "import gym\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "from collections import deque\n",
        "import random\n",
        "from itertools import count\n",
        "import torch.nn.functional as F\n",
        "import matplotlib.pyplot as plt\n",
        "import optuna\n",
        "# from tensorboardX import SummaryWriter\n",
        "\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "\n",
        "class QNetwork(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(QNetwork, self).__init__()\n",
        "\n",
        "        self.fc1 = nn.Linear(6, 64)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.fc_value = nn.Linear(64, 256)\n",
        "        self.fc_adv = nn.Linear(64, 256)\n",
        "\n",
        "        self.value = nn.Linear(256, 1)\n",
        "        self.adv = nn.Linear(256, 3)\n",
        "\n",
        "    def forward(self, state):\n",
        "        y = self.relu(self.fc1(state))\n",
        "        value = self.relu(self.fc_value(y))\n",
        "        adv = self.relu(self.fc_adv(y))\n",
        "\n",
        "        value = self.value(value)\n",
        "        adv = self.adv(adv)\n",
        "\n",
        "        advAverage = torch.mean(adv, dim=1, keepdim=True)\n",
        "        Q = value + adv - advAverage\n",
        "\n",
        "        return Q\n",
        "\n",
        "    def forward_2(self, state):\n",
        "        y = self.relu(self.fc1(state))\n",
        "        value = self.relu(self.fc_value(y))\n",
        "        adv = self.relu(self.fc_adv(y))\n",
        "\n",
        "        value = self.value(value)\n",
        "        adv = self.adv(adv)\n",
        "\n",
        "        advMax = torch.max(adv, dim=1, keepdim=True).values\n",
        "        Q = value + adv - advMax\n",
        "\n",
        "        return Q\n",
        "\n",
        "    def select_action(self, state):\n",
        "        with torch.no_grad():\n",
        "            Q = self.forward(state)\n",
        "            action_index = torch.argmax(Q, dim=1)\n",
        "        return action_index.item()\n",
        "\n",
        "    def select_action_2(self, state):\n",
        "        with torch.no_grad():\n",
        "            Q = self.forward_2(state)\n",
        "            action_index = torch.argmax(Q, dim=1)\n",
        "        return action_index.item()\n",
        "\n",
        "\n",
        "class Memory(object):\n",
        "    def __init__(self, memory_size: int) -> None:\n",
        "        self.memory_size = memory_size\n",
        "        self.buffer = deque(maxlen=self.memory_size)\n",
        "\n",
        "    def add(self, experience) -> None:\n",
        "        self.buffer.append(experience)\n",
        "\n",
        "    def size(self):\n",
        "        return len(self.buffer)\n",
        "\n",
        "    def sample(self, batch_size: int, continuous: bool = True):\n",
        "        if batch_size > len(self.buffer):\n",
        "            batch_size = len(self.buffer)\n",
        "        if continuous:\n",
        "            rand = random.randint(0, len(self.buffer) - batch_size)\n",
        "            return [self.buffer[i] for i in range(rand, rand + batch_size)]\n",
        "        else:\n",
        "            indexes = np.random.choice(np.arange(len(self.buffer)), size=batch_size, replace=False)\n",
        "            return [self.buffer[i] for i in indexes]\n",
        "\n",
        "    def clear(self):\n",
        "        self.buffer.clear()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WftHVOG8Cbr1"
      },
      "source": [
        "Defining the environment and network"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U7G124OtCbZT",
        "outputId": "cecb98a7-1cf7-4821-a912-5356c08dbaf8"
      },
      "outputs": [],
      "source": [
        "env = gym.make('Acrobot-v1')\n",
        "onlineQNetwork = QNetwork().to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Defining the Training Class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "class Training:\n",
        "    def __init__(self, environment,action_selection):\n",
        "        self.environment = environment\n",
        "        self.action_selection=action_selection\n",
        "\n",
        "    def objective(self, trial):\n",
        "        params = {\n",
        "            'initial_epsilon': trial.suggest_loguniform('initial_epsilon', 0.1, 0.2),\n",
        "            'lr': trial.suggest_loguniform('lr', 1e-5, 1e-4),\n",
        "            'batch_size': trial.suggest_categorical('batch_size', [32, 64, 128]),\n",
        "            'replay_size': trial.suggest_categorical('replay_size', [50000, 75000, 100000]),\n",
        "        }\n",
        "        seed = 1\n",
        "        episode_rewards = self.train(seed, params, break_at_threshold=True, action_selection=onlineQNetwork.select_action)\n",
        "        return np.mean(episode_rewards)\n",
        "\n",
        "    def train(self, seed, params, break_at_threshold=False, action_selection=None):\n",
        "        if action_selection is None:\n",
        "            action_selection = self.action_selection\n",
        "        env = self.environment\n",
        "        env.seed(seed)\n",
        "        torch.manual_seed(seed)\n",
        "        np.random.seed(seed)\n",
        "        random.seed(seed)\n",
        "\n",
        "        # onlineQNetwork = QNetwork().to(device)\n",
        "        targetQNetwork = QNetwork().to(device)\n",
        "        targetQNetwork.load_state_dict(onlineQNetwork.state_dict())\n",
        "        optimizer = torch.optim.Adam(onlineQNetwork.parameters(), lr=params['lr'])\n",
        "\n",
        "        GAMMA = 0.99\n",
        "        EXPLORE = 20000\n",
        "        INITIAL_EPSILON = params['initial_epsilon']\n",
        "        FINAL_EPSILON = 0.0001\n",
        "        REPLAY_MEMORY = params['replay_size']\n",
        "        BATCH = params['batch_size']\n",
        "        MAX_EPISODES = 500\n",
        "        UPDATE_STEPS = 4\n",
        "\n",
        "        memory_replay = Memory(REPLAY_MEMORY)\n",
        "\n",
        "        epsilon = INITIAL_EPSILON\n",
        "        learn_steps = 0\n",
        "        begin_learn = False\n",
        "\n",
        "        episode_rewards = []\n",
        "        running_reward = -500\n",
        "        counter = 0\n",
        "\n",
        "        for epoch in range(MAX_EPISODES):\n",
        "            state = env.reset()\n",
        "            episode_reward = 0\n",
        "            done = False\n",
        "            while not done:\n",
        "                p = random.random()\n",
        "                if p < epsilon:\n",
        "                    action = random.randint(0, 1)\n",
        "                else:\n",
        "                    tensor_state = torch.FloatTensor(state).unsqueeze(0).to(device)\n",
        "                    action = action_selection(tensor_state)\n",
        "                next_state, reward, done, _ = env.step(action)\n",
        "                episode_reward += reward\n",
        "\n",
        "                memory_replay.add((state, next_state, action, reward, done))\n",
        "\n",
        "                if memory_replay.size() > 128:\n",
        "                    if not begin_learn:\n",
        "                        print('learn begin!')\n",
        "                        begin_learn = True\n",
        "                    learn_steps += 1\n",
        "                    if learn_steps % UPDATE_STEPS == 0:\n",
        "                        targetQNetwork.load_state_dict(onlineQNetwork.state_dict())\n",
        "\n",
        "                    batch = memory_replay.sample(BATCH, False)\n",
        "                    batch_state, batch_next_state, batch_action, batch_reward, batch_done = zip(*batch)\n",
        "\n",
        "                    batch_state = torch.FloatTensor(batch_state).to(device)\n",
        "                    batch_next_state = torch.FloatTensor(batch_next_state).to(device)\n",
        "                    batch_action = torch.FloatTensor(batch_action).unsqueeze(1).to(device)\n",
        "                    batch_reward = torch.FloatTensor(batch_reward).unsqueeze(1).to(device)\n",
        "                    batch_done = torch.FloatTensor(batch_done).unsqueeze(1).to(device)\n",
        "\n",
        "                    with torch.no_grad():\n",
        "                        onlineQ_next = onlineQNetwork(batch_next_state)\n",
        "                        targetQ_next = targetQNetwork(batch_next_state)\n",
        "                        online_max_action = torch.argmax(onlineQ_next, dim=1, keepdim=True)\n",
        "                        y = batch_reward + (1 - batch_done) * GAMMA * targetQ_next.gather(1, online_max_action.long())\n",
        "\n",
        "                    loss = F.mse_loss(onlineQNetwork(batch_state).gather(1, batch_action.long()), y)\n",
        "                    optimizer.zero_grad()\n",
        "                    loss.backward()\n",
        "                    optimizer.step()\n",
        "\n",
        "                    if epsilon > FINAL_EPSILON:\n",
        "                        epsilon -= (INITIAL_EPSILON - FINAL_EPSILON) / EXPLORE\n",
        "\n",
        "                state = next_state\n",
        "\n",
        "            episode_rewards.append(episode_reward)\n",
        "\n",
        "            running_reward = 0.05 * episode_reward + (1 - 0.05) * running_reward\n",
        "\n",
        "            if running_reward > env.spec.reward_threshold:\n",
        "                if counter < 1:\n",
        "                    print(\"Environment Solved. Running Reward is now {}\".format(running_reward))\n",
        "                    counter += 1\n",
        "                if break_at_threshold:\n",
        "                    break\n",
        "\n",
        "            if epoch % 100 == 0:\n",
        "                print('Seed: {}, Epoch: {}\\tMoving average score: {:.2f}\\t'.format(seed, epoch, episode_reward))\n",
        "\n",
        "        return episode_rewards"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cP1OqREZCQqX"
      },
      "source": [
        "Tuning Hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yIqd7ohanIKM",
        "outputId": "61f5cdd0-895c-44ab-870e-843bb6923ba1"
      },
      "outputs": [],
      "source": [
        "\n",
        "TrainType1=Training(env,onlineQNetwork.select_action)\n",
        "study = optuna.create_study(direction='maximize')\n",
        "\n",
        "# Run the optimization\n",
        "study.optimize(lambda trial: TrainType1.objective(trial), n_trials=3)\n",
        "\n",
        "# Print the best parameters found\n",
        "print(\"Best trial:\")\n",
        "trial = study.best_trial\n",
        "print(\"  Value: \", trial.value)\n",
        "print(\"  Params: \")\n",
        "for key, value in trial.params.items():\n",
        "    print(\"    {}: {}\".format(key, value))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fbyjtqtyCQqY"
      },
      "source": [
        "Running the experiment 5 times and plotting"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 219
        },
        "id": "YRygcNr3ii2n",
        "outputId": "26e65bb7-cd5e-423a-f42c-5d456cd5f2ea"
      },
      "outputs": [],
      "source": [
        "num_seeds = 5\n",
        "all_episode_rewards = []\n",
        "\n",
        "for seed in range(num_seeds):\n",
        "    episode_rewards = TrainType1.train(seed,trial.params)\n",
        "    all_episode_rewards.append(episode_rewards)\n",
        "\n",
        "# Calculate mean and variance across runs for each episode\n",
        "mean_rewards_1 = np.mean(all_episode_rewards, axis=0)\n",
        "variance_rewards_1 = np.var(all_episode_rewards, axis=0)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1b4bbQNnwLoo"
      },
      "source": [
        "## Type 2 DQN\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kwk8Bop_CQqY"
      },
      "source": [
        "Tuning Hyperparamters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7LwBy41uCQqY",
        "outputId": "42b134cc-8a46-41a0-a83e-aafdd37fa244"
      },
      "outputs": [],
      "source": [
        "\n",
        "TrainType2=Training(env,onlineQNetwork.select_action_2)\n",
        "study = optuna.create_study(direction='maximize')\n",
        "\n",
        "# Run the optimization\n",
        "study.optimize(lambda trial: TrainType2.objective(trial), n_trials=3)\n",
        "\n",
        "# Print the best parameters found\n",
        "print(\"Best trial:\")\n",
        "trial2 = study.best_trial\n",
        "print(\"  Value: \", trial2.value)\n",
        "print(\"  Params: \")\n",
        "for key, value in trial2.params.items():\n",
        "    print(\"    {}: {}\".format(key, value))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "Hrur8d0eB7Ql",
        "outputId": "9922a4ba-df02-4a87-9c6a-0ac09d6c83f0"
      },
      "outputs": [],
      "source": [
        "num_seeds = 5\n",
        "all_episode_rewards_2 = []\n",
        "\n",
        "\n",
        "for seed in range(num_seeds):\n",
        "    episode_rewards = TrainType2.train(seed,trial2.params)\n",
        "    all_episode_rewards_2.append(episode_rewards)\n",
        "\n",
        "# Calculate mean and variance across runs for each episode\n",
        "mean_rewards_2 = np.mean(all_episode_rewards_2, axis=0)\n",
        "variance_rewards_2 = np.var(all_episode_rewards_2, axis=0)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "36UZEYXJKC0f"
      },
      "source": [
        "Plotting and comparing the two reward curves"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "JzdF-Xt9KCYy"
      },
      "outputs": [],
      "source": [
        "# Calculate overall variances\n",
        "overall_variance_1 = np.mean(variance_rewards_1)\n",
        "overall_variance_2 = np.mean(variance_rewards_2)\n",
        "\n",
        "if overall_variance_1 > overall_variance_2:\n",
        "    higher_variance_label = 'Mean Return (Type-1, Var:{:.2f}) higher than (Type-2, Var:{:.2f}) '.format(overall_variance_1,overall_variance_2)\n",
        "elif overall_variance_1 < overall_variance_2:\n",
        "    higher_variance_label = 'Mean Return (Type-2) has higher variance (Variance: {:.2f})'.format(overall_variance_2)\n",
        "else:\n",
        "    higher_variance_label = 'Both sets have the same variance (Variance: {:.2f})'.format(overall_variance_1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yCj7bWhBKLX6"
      },
      "source": [
        "Plotting"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 472
        },
        "id": "ehw1nn7RshnR",
        "outputId": "7cf40344-f523-400e-f386-7f259786a4e2"
      },
      "outputs": [],
      "source": [
        "# Plot mean episodic return with shaded regions representing variance for variant 1\n",
        "plt.plot(range(1, len(mean_rewards_1) + 1), mean_rewards_1, label='Mean Return (Type-1)', color='blue')\n",
        "plt.fill_between(range(1, len(mean_rewards_1) + 1), mean_rewards_1 - np.sqrt(variance_rewards_1),\n",
        "                 mean_rewards_1 + np.sqrt(variance_rewards_1), color='blue', alpha=0.1)\n",
        "\n",
        "# Plot mean episodic return with shaded regions representing variance for variant 2\n",
        "plt.plot(range(1, len(mean_rewards_2) + 1), mean_rewards_2, label='Mean Return (Type-2)', color='orange')\n",
        "plt.fill_between(range(1, len(mean_rewards_2) + 1), mean_rewards_2 - np.sqrt(variance_rewards_2),\n",
        "                 mean_rewards_2 + np.sqrt(variance_rewards_2), color='orange', alpha=0.1)\n",
        "\n",
        "plt.text(0.5, 0.05, higher_variance_label, horizontalalignment='center', verticalalignment='center', transform=plt.gca().transAxes)\n",
        "\n",
        "\n",
        "plt.xlabel('Episode')\n",
        "plt.ylabel('Episodic Return')\n",
        "plt.title('Episodic Return vs. Episode Number (Mean and Variance across 5 runs)')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Plotting the moving average for last 100 episodes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# Define a function to calculate mean average and variance for last 100 episodes\n",
        "def calculate_last_100_stats(data):\n",
        "    last_100_mean = np.mean(data[-100:])\n",
        "    last_100_var = np.var(data[-100:])\n",
        "    return last_100_mean, last_100_var\n",
        "\n",
        "# Initialize lists to hold mean rewards and variances for both types\n",
        "mean_rewards_1_last_100 = []\n",
        "variance_rewards_1_last_100 = []\n",
        "mean_rewards_2_last_100 = []\n",
        "variance_rewards_2_last_100 = []\n",
        "\n",
        "# Iterate over episodes\n",
        "for episode in range(1, len(mean_rewards_1) + 1):\n",
        "    # Calculate mean average and variance for last 100 episodes for both types\n",
        "    mean_1_last_100, var_1_last_100 = calculate_last_100_stats(mean_rewards_1[:episode])\n",
        "    mean_2_last_100, var_2_last_100 = calculate_last_100_stats(mean_rewards_2[:episode])\n",
        "    \n",
        "    # Append to lists\n",
        "    mean_rewards_1_last_100.append(mean_1_last_100)\n",
        "    variance_rewards_1_last_100.append(var_1_last_100)\n",
        "    mean_rewards_2_last_100.append(mean_2_last_100)\n",
        "    variance_rewards_2_last_100.append(var_2_last_100)\n",
        "\n",
        "# Plot mean average and variance for last 100 episodes\n",
        "plt.plot(range(1, len(mean_rewards_1_last_100) + 1), mean_rewards_1_last_100, label='Mean Return (Type-1)', color='blue')\n",
        "plt.fill_between(range(1, len(mean_rewards_1_last_100) + 1), mean_rewards_1_last_100 - np.sqrt(variance_rewards_1_last_100),\n",
        "                 mean_rewards_1_last_100 + np.sqrt(variance_rewards_1_last_100), color='blue', alpha=0.1)\n",
        "\n",
        "plt.plot(range(1, len(mean_rewards_2_last_100) + 1), mean_rewards_2_last_100, label='Mean Return (Type-2)', color='orange')\n",
        "plt.fill_between(range(1, len(mean_rewards_2_last_100) + 1), mean_rewards_2_last_100 - np.sqrt(variance_rewards_2_last_100),\n",
        "                 mean_rewards_2_last_100 + np.sqrt(variance_rewards_2_last_100), color='orange', alpha=0.1)\n",
        "\n",
        "plt.xlabel('Episode')\n",
        "plt.ylabel('Mean Episodic Return (Last 100 Episodes)')\n",
        "plt.title('Mean Episodic Return and Variance (Last 100 Episodes) vs. Episode Number')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "undefined.undefined.undefined"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
